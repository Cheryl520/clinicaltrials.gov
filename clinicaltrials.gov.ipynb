{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000317eb-23e5-4267-a5f8-85c17d73202a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\86136\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\86136\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\86136\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\86136\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\86136\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\86136\\anaconda3\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\86136\\anaconda3\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\86136\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\86136\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\86136\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\86136\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\86136\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\86136\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b3933d1-e47b-4508-9604-656dfd9d59c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of the studies: 4719; Total pages to be fetched: 48; Total parquet files to be saved: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|██████████████████████████████████████████████████████████████████| 48/48 [00:59<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "import requests  # Library for making HTTP requests\n",
    "import pandas as pd  # Library for data manipulation and analysis\n",
    "import uuid  # Library for generating unique identifiers\n",
    "from datetime import datetime  # Library for handling date and time\n",
    "from pathlib import Path  # Library for working with file system paths\n",
    "from tqdm import tqdm  # Library for displaying progress bars\n",
    "import logging  # Library for logging messages\n",
    "\n",
    "# Setup logging configuration\n",
    "logging.basicConfig(filename='fetch_studies.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def fetch_data(base_url, params):\n",
    "    \"\"\" \n",
    "    Function to fetch data from the API and return JSON response. \n",
    "    \n",
    "    Args:\n",
    "        base_url (str): The base URL of the API endpoint.\n",
    "        params (dict): Parameters to be passed in the request.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: The JSON response if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()  # Check for any HTTP errors\n",
    "        return response.json()  # Return the JSON response\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Request failed: {e}\")  # Log error message\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_to_parquet(data, output_folder, pipeline_run_id, pipeline_start_timestamp, file_part):\n",
    "    \"\"\" \n",
    "    Function to save data to a Parquet file and return the file path. \n",
    "    \n",
    "    Args:\n",
    "        data (list): List of dictionaries representing the data.\n",
    "        output_folder (str or Path): Path to the output folder.\n",
    "        pipeline_run_id (str): Unique ID for each pipeline run.\n",
    "        pipeline_start_timestamp (timestamp): Timestamp when the pipeline run starts.\n",
    "        file_part (int): Part number of the file.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the saved Parquet file.\n",
    "    \"\"\"\n",
    "    # Convert data to a pandas DataFrame\n",
    "    df = pd.json_normalize(data)\n",
    "    df['pipeline_run_id'] = pipeline_run_id\n",
    "    df['pipeline_start_timestamp'] = pipeline_start_timestamp\n",
    "    # Construct the output file path\n",
    "    output_file = output_folder / f\"clinical_trials_run_{pipeline_run_id}_part{file_part}.parquet\"\n",
    "    # Save the DataFrame to a Parquet file\n",
    "    df.to_parquet(output_file, index=False)\n",
    "    return output_file  # Return the file path\n",
    "\n",
    "def fetch_studies(from_date=None, pageSize=100, pagePerParquet=50):\n",
    "    \"\"\" \n",
    "    Function to fetch studies from the API and save them to Parquet files. \n",
    "    \n",
    "    Args:\n",
    "        from_date (str, optional): Specifies the date in YYYY-MM-DD format from which to fetch studies. If provided, the function will pull only studies that have been updated on or after this date. Defaults to None.\n",
    "        pageSize (int, optional): Number of studies to fetch per page. Defaults to 100.\n",
    "        pagePerParquet (int, optional): Number of pages to save per Parquet file. Defaults to 50.\n",
    "    \"\"\"\n",
    "    # Create the output folder if it does not exist\n",
    "    output_folder = Path.cwd() / \"output_parquet\"\n",
    "    output_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    base_url = \"https://clinicaltrials.gov/api/v2/studies\"  # Base URL of the API endpoint\n",
    "    params = {\"format\": \"json\", \"countTotal\": \"true\", \"pageSize\": pageSize}  # Parameters for the API request\n",
    "    if from_date:\n",
    "        params[\"query.term\"] = f\"AREA[LastUpdateSubmitDate]RANGE[{from_date},MAX]\"  # Add date filter if provided\n",
    "\n",
    "    # Fetch initial response to determine total count of studies\n",
    "    initial_response = fetch_data(base_url, params)\n",
    "  \n",
    "    # Extract total count of studies and calculate total pages and files\n",
    "    total_count = initial_response.get('totalCount', 0)\n",
    "    total_pages = (total_count // pageSize) + (total_count % pageSize > 0)\n",
    "    total_files = (total_pages // pagePerParquet) + (total_pages % pagePerParquet > 0)\n",
    "    if total_count == 0:\n",
    "        logging.info('Request succeeded but no studies fetched.')  # Log info message\n",
    "        print('Request succeeded but no studies fetched.')\n",
    "    else:\n",
    "        print(f\"Total count of the studies: {total_count}; Total pages to be fetched: {total_pages}; Total parquet files to be saved: {total_files}\")\n",
    "\n",
    "    # Initialize variables for tracking progress\n",
    "    all_studies = []\n",
    "    page_count = 0\n",
    "    file_part = 1\n",
    "    pipeline_run_id = str(uuid.uuid4())\n",
    "    pipeline_start_timestamp = datetime.now()\n",
    "\n",
    "    # If the response is not empty, loop through pages to fetch all studies\n",
    "    if total_count != 0:    \n",
    "        pbar = tqdm(total=total_pages, desc=\"Fetching pages\", leave=True, position=0)\n",
    "        nextPageToken = None\n",
    "        while True:\n",
    "            # Fetch data from the API\n",
    "            if nextPageToken:\n",
    "                params[\"pageToken\"] = nextPageToken\n",
    "            json_response = fetch_data(base_url, params)\n",
    "    \n",
    "            # Extract studies from the JSON response\n",
    "            page_studies = json_response.get('studies', [])\n",
    "            nextPageToken = json_response.get('nextPageToken')\n",
    "            all_studies.extend(page_studies)\n",
    "            page_count += 1\n",
    "            pbar.update(1)  # Update progress bar\n",
    "    \n",
    "            # Save to Parquet every pagePerParquet pages or at the last page\n",
    "            if page_count % pagePerParquet == 0 or not nextPageToken:\n",
    "                output_file = save_to_parquet(all_studies, output_folder, pipeline_run_id, pipeline_start_timestamp, file_part)\n",
    "                logging.info(f\"Data written to {output_file}\")  # Log file path\n",
    "                all_studies = []  # Clear list for the next batch\n",
    "                file_part += 1\n",
    "    \n",
    "            if not nextPageToken:\n",
    "                break  # Exit loop if there are no more pages\n",
    "\n",
    "        # Close the progress bar\n",
    "        pbar.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage: Fetch studies updated since April 10, 2024, and save every 100 pages to 1 parquet file\n",
    "    fetch_studies('2024-04-10',pagePerParquet=100)\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fae91e0-f202-450e-bc2c-6d88410d2ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
